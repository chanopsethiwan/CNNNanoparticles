{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #choose whether to use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder'  #path to the folder containing the images\n",
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\PastData\\helical_size'\n",
    "first_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\first_half\\first_quarter'\n",
    "second_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\first_half\\second_quarter'\n",
    "third_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\second_half\\third_quarter'\n",
    "fourth_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\second_half\\fourth_quarter'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-input dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, first_dir, second_dir, third_dir, fourth_dir, transform=None):\n",
    "        self.first_dir = first_dir\n",
    "        self.second_dir = second_dir\n",
    "        self.third_dir = third_dir\n",
    "        self.fourth_dir = fourth_dir\n",
    "        self.transform = transform\n",
    "        # self.images1 = sorted(os.listdir(first_dir))\n",
    "        self.images1 = sorted(os.listdir(first_dir), key=self.custom_sort_key)\n",
    "        self.images2 = sorted(os.listdir(second_dir), key=self.custom_sort_key)\n",
    "        self.images3 = sorted(os.listdir(third_dir), key=self.custom_sort_key)\n",
    "        self.images4 = sorted(os.listdir(fourth_dir), key=self.custom_sort_key)\n",
    "        self.labels1 = [self.extract_label(img) for img in self.images1]\n",
    "        self.labels2 = [self.extract_label(img) for img in self.images2]\n",
    "        self.labels3 = [self.extract_label(img) for img in self.images3]\n",
    "        self.labels4 = [self.extract_label(img) for img in self.images4]\n",
    "        \n",
    "        #input verification\n",
    "        assert len(self.images1) == len(self.images2) == len(self.images3) == len(self.images4)\n",
    "\n",
    "        self.length = len(self.images1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        # return len(self.images1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name1 = os.path.join(self.first_dir, self.images1[idx])\n",
    "        image1 = Image.open(img_name1)\n",
    "        img_name2 = os.path.join(self.second_dir, self.images2[idx])\n",
    "        image2 = Image.open(img_name2)\n",
    "        img_name3 = os.path.join(self.third_dir, self.images3[idx])\n",
    "        image3 = Image.open(img_name3)\n",
    "        img_name4 = os.path.join(self.fourth_dir, self.images4[idx])\n",
    "        image4 = Image.open(img_name4)\n",
    "    \n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "            image3 = self.transform(image3)\n",
    "            image4 = self.transform(image4)\n",
    "        \n",
    "        # print(self.labels1)\n",
    "        # print(self.labels2)\n",
    "        # print(self.labels3)\n",
    "        # print(self.labels4)\n",
    "\n",
    "        label1 = self.labels1[idx]\n",
    "        label2 = self.labels2[idx]\n",
    "        label3 = self.labels3[idx]\n",
    "        label4 = self.labels4[idx]\n",
    "        return image1, image2, image3, image4, label1, label2, label3, label4\n",
    "\n",
    "    def custom_sort_key(self, item):\n",
    "        # Extract numbers after 'Fig_' and 't-'\n",
    "        fig_number = float(item.split('Fig_')[1].split('__')[0])\n",
    "        t_number = float(item.split('t-')[1].split('_')[0])\n",
    "        \n",
    "        return fig_number, t_number\n",
    "    \n",
    "    def extract_label(self, img_name):\n",
    "        # Assuming that the label is part of the filename before the first underscore\n",
    "        label = float(img_name[-17:-5]) #this is the right code\n",
    "        # label = str(img_name)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950\n",
      "2962\n",
      "988\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.45), (0.25))]) \n",
    "\n",
    "custom_dataset = CustomImageDataset(first_dir=first_dir, second_dir=second_dir, third_dir=third_dir, fourth_dir=fourth_dir, transform=data_transform)\n",
    "\n",
    "# # Accessing the data\n",
    "# for img, label in custom_dataset:\n",
    "#     print(f\"Image shape: {img.shape}, Label: {label}\")\n",
    "\n",
    "print(len(custom_dataset))\n",
    "\n",
    "# train_set, val_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*0.75), int(len(custom_dataset)*0.15), int(len(custom_dataset)*0.100056)]) #splits data into training, validation and test sets\n",
    "train_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*0.75), int(len(custom_dataset)*0.25013)])\n",
    "print(len(train_set))\n",
    "# print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.49329 6.343753 10.03911 11.13706\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "num_epochs = 30\n",
    "batch_size = 1\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# allData = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "# # print(allData)\n",
    "# for (images1, images2, images3, images4, labels1, labels2, labels3, labels4) in allData:\n",
    "#     print(labels1, labels2, labels3, labels4)\n",
    "\n",
    "\n",
    "train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "for (images1, images2, images3, images4, labels1, labels2, labels3, labels4) in train:\n",
    "    print(labels1.item(), labels2.item(), labels3.item(), labels4.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): # note need to find out image size\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,30,20, padding='same') #in_channels, out_channels, kernel_size\n",
    "        self.normalise1 = nn.BatchNorm2d(30)\n",
    "        # self.pool = nn.MaxPool2d(5,5) #kernel_size, stride (shift x pixel to the right)\n",
    "        self.pool1 = nn.AvgPool2d(10, stride=10)\n",
    "        # self.pool1 = nn.MaxPool2d(10, stride=10)\n",
    "        self.conv2 = nn.Conv2d(30, 30, 20, padding='same')\n",
    "        # self.normalise2 = nn.BatchNorm2d(16)\n",
    "        # self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        # self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(30, 30, 10, padding='same')\n",
    "        # self.normalise3 = nn.BatchNorm2d(32) \n",
    "        self.conv4 = nn.Conv2d(30, 30, 10, padding='same')\n",
    "        # self.fc1 = nn.Linear(16*3*3, 120) # 3x3 is the size of the image after 2 conv layers, 16 is the number of channels, 120 is the number of nodes in the hidden layer\n",
    "        # self.fc2 = nn.Linear(120,84)\n",
    "        # self.fc3 = nn.Linear(60, 1)\n",
    "        self.fc0 = nn.Linear(30, 10)\n",
    "        self.fc = nn.Linear(32*5*5, 1)\n",
    "        self.fc1 = nn.Linear(32*5*5, 400)\n",
    "        self.fc2 = nn.Linear(400,200)\n",
    "        self.fc3 = nn.Linear(200,4)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, z, a):\n",
    "        print(x.shape)\n",
    "        x = self.pool1(F.relu(self.normalise1(self.conv1(x)))) \n",
    "        x = self.pool1(F.relu(self.normalise1(self.conv2(x)))) \n",
    "        x = x.view(-1, 30)  #flatten\n",
    "\n",
    "        x = self.fc0(x)\n",
    "        print(x.shape)\n",
    "        y = self.pool1(F.relu(self.normalise1(self.conv1(y)))) \n",
    "        y = self.pool1(F.relu(self.normalise1(self.conv2(y)))) \n",
    "        print(y.shape)\n",
    "        y = y.view(-1, 30)  #flatten\n",
    "        y = self.fc0(y)\n",
    "        print(y.shape)\n",
    "        z = self.pool1(F.relu(self.normalise1(self.conv1(z)))) \n",
    "        z = z.view(-1, 30)  #flatten\n",
    "        z = self.fc0(z)\n",
    "        a = self.pool1(F.relu(self.normalise1(self.conv1(a)))) \n",
    "        a = a.view(-1, 30)  #flatten\n",
    "        a = self.fc0(a)\n",
    "\n",
    "        combined = torch.cat((x,y,z,a), dim=-1)\n",
    "        print(combined.shape)\n",
    "        combined = self.pool1(F.relu(self.normalise1(self.conv3(combined)))) \n",
    "        combined = self.pool1(F.relu(self.normalise1(self.conv4(combined)))) \n",
    "        combined = combined.view(-1, 32*5*5)  #flatten\n",
    "        combined = F.relu(self.fc1(combined))\n",
    "        combined = F.relu(self.fc2(combined))\n",
    "        combined = self.fc3(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100, 100])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 30, 1, 1])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1 but got size 100 for tensor number 2 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m labels4 \u001b[38;5;241m=\u001b[39m labels4\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#forward\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images1, images2, images3, images4)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(labels)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m labels1 \u001b[38;5;241m=\u001b[39m labels1\u001b[38;5;241m.\u001b[39mfloat() \n",
      "File \u001b[1;32mc:\\Users\\cs2036\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cs2036\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[117], line 48\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x, y, z, a)\u001b[0m\n\u001b[0;32m     45\u001b[0m a \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m#flatten\u001b[39;00m\n\u001b[0;32m     46\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc0(a)\n\u001b[1;32m---> 48\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x,y,z,a), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     50\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalise1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(combined)))) \n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1 but got size 100 for tensor number 2 in the list."
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "#training loop\n",
    "n_total_steps = len(train)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (images1, images2, images3, images4, labels1, labels2, labels3, labels4) in enumerate(train):\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        images3 = images3.to(device)    \n",
    "        images4 = images4.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        labels3 = labels3.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "\n",
    "        #forward\n",
    "        outputs = model(images1, images2, images3, images4)\n",
    "        # print(labels)\n",
    "        labels1 = labels1.float() \n",
    "        labels2 = labels2.float()\n",
    "        labels3 = labels3.float()\n",
    "        labels4 = labels4.float()\n",
    "        labels = torch.cat((labels1, labels2, labels3, labels4), dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 1000 ==0:\n",
    "        # print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "        total_loss += loss.item()\n",
    "    print(f'epoch {epoch+1}/{num_epochs}, average loss = {total_loss/len(train):.4f}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
