{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #choose whether to use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder'  #path to the folder containing the images\n",
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\PastData\\helical_size'\n",
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\second_half'\n",
    "data_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\size_folder' #path to folder for chappy computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]) #this excludes folder as well\n",
    "        self.labels = [self.extract_label(img) for img in self.images]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "    def extract_label(self, img_name):\n",
    "        # Assuming that the label is part of the filename before the first underscore\n",
    "        label = float(img_name[-17:-5]) #this is the right code\n",
    "        # label = img_name\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15800\n",
      "11850\n",
      "3950\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.45), (0.25))]) \n",
    "\n",
    "custom_dataset = CustomImageDataset(root_dir=data_dir, transform=data_transform)\n",
    "\n",
    "# # Accessing the data\n",
    "# for img, label in custom_dataset:\n",
    "#     print(f\"Image shape: {img.shape}, Label: {label}\")\n",
    "\n",
    "print(len(custom_dataset))\n",
    "# train_set, val_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*0.75), int(len(custom_dataset)*0.15), int(len(custom_dataset)*0.100056)]) #splits data into training, validation and test sets\n",
    "train_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*0.75), int(len(custom_dataset)*0.25003)])\n",
    "print(len(train_set))\n",
    "# print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "num_epochs = 30\n",
    "# num_epochs = 60\n",
    "batch_size = 1\n",
    "learning_rate = 0.0005\n",
    "# learning_rate = 0.001\n",
    "\n",
    "train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "# val = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=100, patch_size=16, num_hiddens=512):\n",
    "        super().__init__()\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "        img_size, patch_size = _make_tuple(img_size), _make_tuple(patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        self.conv = nn.LazyConv2d(num_hiddens, kernel_size=patch_size,\n",
    "                                  stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Output shape: (batch size, no. of patches, no. of channels)\n",
    "        return self.conv(X).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP inside vision transformer encoder\n",
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(self.dense1(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vision transformer encoder block\n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, norm_shape, mlp_num_hiddens,\n",
    "                 num_heads, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(norm_shape)\n",
    "        self.attention = d2l.MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.ln2 = nn.LayerNorm(norm_shape)\n",
    "        self.mlp = ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens=None):\n",
    "        X = X + self.attention(*([self.ln1(X)] * 3), valid_lens)\n",
    "        return X + self.mlp(self.ln2(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
