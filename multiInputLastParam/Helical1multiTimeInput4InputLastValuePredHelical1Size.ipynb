{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #choose whether to use gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder'  #path to the folder containing the images\n",
    "# data_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\PastData\\helical_size'\n",
    "# first_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\first_half\\first_quarter'     #Bruno's computer\n",
    "# second_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\first_half\\second_quarter'\n",
    "# third_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\second_half\\third_quarter'\n",
    "# fourth_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\Outputs_Grayscale_Labelled_Images_Sizes\\size_folder\\second_half\\fourth_quarter'\n",
    "\n",
    "# first_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\6ClassHelical_1\\first_fifth'\n",
    "# second_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\6ClassHelical_1\\second_fifth'\n",
    "# third_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\6ClassHelical_1\\third_fifth'\n",
    "# fourth_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\6ClassHelical_1\\fourth_fifth'\n",
    "# fifth_dir = r'C:\\Users\\Public\\PartIIB project 2023_2024\\Image collection without reaction\\00AgNO3_mole_fraction\\6ClassHelical_1\\fifth_fifth'\n",
    "\n",
    "# first_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\size_folder\\first_half\\first_quarter' #chappy computer\n",
    "# second_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\size_folder\\first_half\\second_quarter'\n",
    "# third_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\size_folder\\second_half\\third_quarter'\n",
    "# fourth_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\size_folder\\second_half\\fourth_quarter'\n",
    "\n",
    "first_dir = r\"C:\\Users\\Chappyyyyyy\\Documents\\helical1DataSelector\\train\\interval_1\"\n",
    "second_dir = r\"C:\\Users\\Chappyyyyyy\\Documents\\helical1DataSelector\\train\\interval_2\"\n",
    "third_dir = r\"C:\\Users\\Chappyyyyyy\\Documents\\helical1DataSelector\\train\\interval_3\"\n",
    "fourth_dir = r\"C:\\Users\\Chappyyyyyy\\Documents\\helical1DataSelector\\train\\interval_4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-input dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, first_dir, second_dir, third_dir, fourth_dir, transform=None):\n",
    "        self.first_dir = first_dir\n",
    "        self.second_dir = second_dir\n",
    "        self.third_dir = third_dir\n",
    "        self.fourth_dir = fourth_dir\n",
    "        self.transform = transform\n",
    "        # self.images1 = sorted(os.listdir(first_dir))\n",
    "        self.images1 = sorted(os.listdir(first_dir), key=self.custom_sort_key)\n",
    "        self.images2 = sorted(os.listdir(second_dir), key=self.custom_sort_key)\n",
    "        self.images3 = sorted(os.listdir(third_dir), key=self.custom_sort_key)\n",
    "        self.images4 = sorted(os.listdir(fourth_dir), key=self.custom_sort_key)\n",
    "        # self.labels1 = [self.extract_label(img) for img in self.images1]\n",
    "        # self.labels2 = [self.extract_label(img) for img in self.images2]\n",
    "        # self.labels3 = [self.extract_label(img) for img in self.images3]\n",
    "        # self.labels4 = [self.extract_label(img) for img in self.images4]\n",
    "        self.labels4 = [self.extract_label(img) for img in self.images4]\n",
    "        self.cuttime = [self.extract_cuttime(img) for img in self.images4]\n",
    "        \n",
    "        #input verification\n",
    "        assert len(self.images1) == len(self.images2) == len(self.images3) == len(self.images4) \n",
    "\n",
    "        self.length = len(self.images1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        # return len(self.images1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name1 = os.path.join(self.first_dir, self.images1[idx])\n",
    "        image1 = Image.open(img_name1)\n",
    "        img_name2 = os.path.join(self.second_dir, self.images2[idx])\n",
    "        image2 = Image.open(img_name2)\n",
    "        img_name3 = os.path.join(self.third_dir, self.images3[idx])\n",
    "        image3 = Image.open(img_name3)\n",
    "        img_name4 = os.path.join(self.fourth_dir, self.images4[idx])\n",
    "        image4 = Image.open(img_name4)\n",
    "\n",
    "    \n",
    "        if self.transform:\n",
    "            image1 = self.transform(image1)\n",
    "            image2 = self.transform(image2)\n",
    "            image3 = self.transform(image3)\n",
    "            image4 = self.transform(image4)\n",
    "        \n",
    "        label4 = self.labels4[idx]\n",
    "        cuttime = self.cuttime[idx]\n",
    "        return image1, image2, image3, image4, label4, cuttime\n",
    "\n",
    "    def custom_sort_key(self, item):\n",
    "        # Extract numbers after 'Fig_' and 't-'\n",
    "        fig_number = float(item.split('Fig_')[1].split('__')[0])\n",
    "        t_number = float(item.split('t-')[1].split('_')[0])\n",
    "        \n",
    "        return fig_number, t_number\n",
    "    \n",
    "    def extract_label(self, img_name):\n",
    "        # Assuming that the label is part of the filename before the first underscore\n",
    "        # label = str(img_name)\n",
    "        # print(label)\n",
    "\n",
    "        label = float(img_name[-17:-5]) #this is the right code\n",
    "        # print(label)\n",
    "\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def extract_cuttime(self, img_name):\n",
    "        cuttime = float(img_name.split(\"t-\")[1].split(\"_\")[0])\n",
    "        return cuttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705\n",
      "705\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "transforms.Normalize((0.45), (0.225))]) \n",
    "\n",
    "custom_dataset = CustomImageDataset(first_dir=first_dir, second_dir=second_dir, third_dir=third_dir, fourth_dir=fourth_dir, transform=data_transform)\n",
    "\n",
    "# # Accessing the data\n",
    "# for img, label in custom_dataset:\n",
    "#     print(f\"Image shape: {img.shape}, Label: {label}\")\n",
    "\n",
    "print(len(custom_dataset))\n",
    "\n",
    "# train_set, val_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*0.75), int(len(custom_dataset)*0.15), int(len(custom_dataset)*0.100056)]) #splits data into training, validation and test sets\n",
    "train_set, test_set = random_split(custom_dataset, [int(len(custom_dataset)*1), int(len(custom_dataset)*0)])\n",
    "print(len(train_set))\n",
    "# print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.799365 170.0\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "num_epochs = 60\n",
    "batch_size = 1\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "# test = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "for (images1, images2, images3, images4, labels4, cuttime) in train:\n",
    "    print(labels4.item(), cuttime.item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): # note need to find out image size\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,8,10, padding='same') #in_channels, out_channels, kernel_size\n",
    "        self.normalise1 = nn.BatchNorm2d(8)\n",
    "        # self.pool1 = nn.AvgPool2d(10, stride=10)\n",
    "        self.pool1 = nn.AvgPool2d(10, stride=10)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 10, padding='same')\n",
    "        self.normalise2 = nn.BatchNorm2d(16)\n",
    "        # self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        self.pool3 = nn.AvgPool2d(1, stride=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 10, padding='same')\n",
    "        self.normalise3 = nn.BatchNorm2d(32) \n",
    "        self.conv4 = nn.Conv2d(32, 32, 10, padding='same')\n",
    "        self.fc0 = nn.Linear(32*5*5, 400)\n",
    "\n",
    "        self.convcomb1 = nn.Conv2d(1, 16, 20, padding='same')\n",
    "        self.convcomb2 = nn.Conv2d(16, 32, 20, padding='same')\n",
    "        \n",
    "        self.fc1 = nn.Linear(3200+40, 1600)\n",
    "        self.fc2 = nn.Linear(1600,800)\n",
    "        self.fc3 = nn.Linear(800,400)\n",
    "        self.fc4 = nn.Linear(400,200)\n",
    "        self.fc5 = nn.Linear(200,50)\n",
    "        self.fc6 = nn.Linear(50,1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(1,1)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, z, a, t):\n",
    "        x = self.pool1(F.relu(self.normalise1(self.conv1(x)))) \n",
    "        x = self.pool2(F.relu(self.normalise2(self.conv2(x)))) \n",
    "        x = self.pool3(F.relu(self.normalise3(self.conv3(x))))\n",
    "        x = self.pool3(F.relu(self.normalise3(self.conv4(x))))\n",
    "        x = torch.flatten(x)  #flatten\n",
    "        # print(x.shape)\n",
    "        x = self.fc0(x)\n",
    "\n",
    "        y = self.pool1(F.relu(self.normalise1(self.conv1(y)))) \n",
    "        y = self.pool2(F.relu(self.normalise2(self.conv2(y)))) \n",
    "        y = self.pool3(F.relu(self.normalise3(self.conv3(y))))\n",
    "        y = self.pool3(F.relu(self.normalise3(self.conv4(y))))\n",
    "        y = torch.flatten(y)  #flatten\n",
    "        # print(y.shape)\n",
    "        y = self.fc0(y)\n",
    "\n",
    "        z = self.pool1(F.relu(self.normalise1(self.conv1(z))))\n",
    "        z = self.pool2(F.relu(self.normalise2(self.conv2(z)))) \n",
    "        z = self.pool3(F.relu(self.normalise3(self.conv3(z))))\n",
    "        z = self.pool3(F.relu(self.normalise3(self.conv4(z))))\n",
    "        z = torch.flatten(z)  #flatten\n",
    "        z = self.fc0(z)\n",
    "\n",
    "        a = self.pool1(F.relu(self.normalise1(self.conv1(a)))) \n",
    "        a = self.pool2(F.relu(self.normalise2(self.conv2(a))))\n",
    "        a = self.pool3(F.relu(self.normalise3(self.conv3(a))))\n",
    "        a = self.pool3(F.relu(self.normalise3(self.conv4(a))))\n",
    "        a = torch.flatten(a) #flatten\n",
    "        a = self.fc0(a)\n",
    "        # print(a.shape)\n",
    "\n",
    "        combined = torch.stack([x,y,z,a])\n",
    "        combined = combined.unsqueeze(0)\n",
    "        combined= combined.unsqueeze(0)\n",
    "        # print(combined.shape)\n",
    "        # print(combined)\n",
    "        combined = self.pool2(F.relu(self.normalise2(self.convcomb1(combined)))) \n",
    "        # print(combined.shape)\n",
    "        # print(combined)\n",
    "        combined = self.pool2(F.relu(self.normalise3(self.convcomb2(combined)))) \n",
    "        # combined = combined.view(-1, 32*5*5)  #flatten\n",
    "        combined = torch.flatten(combined)\n",
    "        # print(combined.shape)\n",
    "        t = self.fc(t)\n",
    "        t = torch.flatten(t.repeat(1,40))\n",
    "        # print(t.shape)\n",
    "        # print(t)\n",
    "        combined = torch.cat((combined, t), 0)\n",
    "        # print(combined.shape)\n",
    "        combined = F.relu(self.fc1(combined))\n",
    "        combined = F.relu(self.fc2(combined))\n",
    "        combined = F.relu(self.fc3(combined))\n",
    "        combined = F.relu(self.fc4(combined))\n",
    "        combined = F.relu(self.fc5(combined))\n",
    "        combined = self.fc6(combined)\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chappyyyyyy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:1009.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/60, average loss = 12.4273\n",
      "epoch 2/60, average loss = 5.5768\n",
      "epoch 3/60, average loss = 2.3850\n",
      "epoch 4/60, average loss = 1.1483\n",
      "epoch 5/60, average loss = 1.4557\n",
      "epoch 6/60, average loss = 0.5657\n",
      "epoch 7/60, average loss = 0.4603\n",
      "epoch 8/60, average loss = 0.6953\n",
      "epoch 9/60, average loss = 0.5712\n",
      "epoch 10/60, average loss = 0.3407\n",
      "epoch 11/60, average loss = 0.5432\n",
      "epoch 12/60, average loss = 0.3554\n",
      "epoch 13/60, average loss = 0.4267\n",
      "epoch 14/60, average loss = 0.3224\n",
      "epoch 15/60, average loss = 0.3628\n",
      "epoch 16/60, average loss = 0.4456\n",
      "epoch 17/60, average loss = 0.2327\n",
      "epoch 18/60, average loss = 0.2727\n",
      "epoch 19/60, average loss = 0.2552\n",
      "epoch 20/60, average loss = 0.2874\n",
      "epoch 21/60, average loss = 0.2222\n",
      "epoch 22/60, average loss = 0.2681\n",
      "epoch 23/60, average loss = 0.2139\n",
      "epoch 24/60, average loss = 0.2718\n",
      "epoch 25/60, average loss = 0.1564\n",
      "epoch 26/60, average loss = 0.2464\n",
      "epoch 27/60, average loss = 0.6568\n",
      "epoch 28/60, average loss = 0.2013\n",
      "epoch 29/60, average loss = 0.1142\n",
      "epoch 30/60, average loss = 0.1410\n",
      "epoch 31/60, average loss = 0.1661\n",
      "epoch 32/60, average loss = 0.1784\n",
      "epoch 33/60, average loss = 0.1480\n",
      "epoch 34/60, average loss = 0.1448\n",
      "epoch 35/60, average loss = 0.1713\n",
      "epoch 36/60, average loss = 0.1557\n",
      "epoch 37/60, average loss = 0.5073\n",
      "epoch 38/60, average loss = 0.0865\n",
      "epoch 39/60, average loss = 0.1164\n",
      "epoch 40/60, average loss = 0.1260\n",
      "epoch 41/60, average loss = 0.2308\n",
      "epoch 42/60, average loss = 0.1524\n",
      "epoch 43/60, average loss = 0.0793\n",
      "epoch 44/60, average loss = 0.1163\n",
      "epoch 45/60, average loss = 0.1049\n",
      "epoch 46/60, average loss = 0.1337\n",
      "epoch 47/60, average loss = 0.2098\n",
      "epoch 48/60, average loss = 0.0889\n",
      "epoch 49/60, average loss = 0.0692\n",
      "epoch 50/60, average loss = 0.0989\n",
      "epoch 51/60, average loss = 0.1307\n",
      "epoch 52/60, average loss = 0.0988\n",
      "epoch 53/60, average loss = 0.1105\n",
      "epoch 54/60, average loss = 0.1292\n",
      "epoch 55/60, average loss = 0.1120\n",
      "epoch 56/60, average loss = 0.0853\n",
      "epoch 57/60, average loss = 0.1159\n",
      "epoch 58/60, average loss = 0.0993\n",
      "epoch 59/60, average loss = 0.0916\n",
      "epoch 60/60, average loss = 0.1330\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "#training loop\n",
    "n_total_steps = len(train)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (images1, images2, images3, images4, labels4, cuttimes) in enumerate(train):\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        images3 = images3.to(device)    \n",
    "        images4 = images4.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        cuttimes = cuttimes.to(device).float()\n",
    "\n",
    "        #forward\n",
    "        outputs = model(images1, images2, images3, images4, cuttimes)\n",
    "        # print(labels)\n",
    "        labels4 = labels4.float()\n",
    "        loss = criterion(outputs, labels4)\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 1000 ==0:\n",
    "        # print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "        total_loss += loss.item()\n",
    "    print(f'epoch {epoch+1}/{num_epochs}, average loss = {total_loss/len(train):.4f}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m test_third_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mChappyyyyyy\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mallHelicalDataSelector\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtestVal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minterval_3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m test_fourth_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mChappyyyyyy\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mallHelicalDataSelector\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtestVal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minterval_4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomImageDataset(first_dir\u001b[38;5;241m=\u001b[39mtest_first_dir, second_dir\u001b[38;5;241m=\u001b[39mtest_second_dir, third_dir\u001b[38;5;241m=\u001b[39mtest_third_dir, fourth_dir\u001b[38;5;241m=\u001b[39mtest_fourth_dir, transform\u001b[38;5;241m=\u001b[39mdata_transform)\n\u001b[0;32m      7\u001b[0m test \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# no need to calculate gradient\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mCustomImageDataset.__init__\u001b[1;34m(self, first_dir, second_dir, third_dir, fourth_dir, transform)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuttime \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_cuttime(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages4]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#input verification\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages3) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages4) \n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages1)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "test_first_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\allHelicalDataSelector\\testVal\\interval_1'\n",
    "test_second_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\allHelicalDataSelector\\testVal\\interval_2'\n",
    "test_third_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\allHelicalDataSelector\\testVal\\interval_3'\n",
    "test_fourth_dir = r'C:\\Users\\Chappyyyyyy\\Documents\\allHelicalDataSelector\\testVal\\interval_4'\n",
    "test_dataset = CustomImageDataset(first_dir=test_first_dir, second_dir=test_second_dir, third_dir=test_third_dir, fourth_dir=test_fourth_dir, transform=data_transform)\n",
    "test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "with torch.no_grad(): # no need to calculate gradient\n",
    "    squared_difference = 0\n",
    "    predicted_size_data = []\n",
    "    CFD_size_data = []\n",
    "    underestimate = []\n",
    "    overestimate = []\n",
    "    for (images1, images2, images3, images4, labels4, cuttimes) in test:\n",
    "        images1 = images1.to(device)\n",
    "        images2 = images2.to(device)\n",
    "        images3 = images3.to(device)    \n",
    "        images4 = images4.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        cuttimes = cuttimes.to(device).float()\n",
    "\n",
    "        #forward\n",
    "        outputs = model(images1, images2, images3, images4, cuttimes)\n",
    "    \n",
    "        # print(predictions)\n",
    "        squared_difference += (float(outputs) - labels4) ** 2\n",
    "        predicted_size_data.append(float(outputs))\n",
    "        CFD_size_data.append(float(labels4))\n",
    "        if float(outputs) < 0.5 * float(labels4):\n",
    "            underestimate.append(float(outputs))\n",
    "        if float(outputs) > 1.5 * float(labels4):\n",
    "            overestimate.append(float(outputs))\n",
    "        \n",
    "    rmse = torch.sqrt(squared_difference / (len(test)))\n",
    "    print(f'RMSE = {rmse}')\n",
    "    # print(predicted_size_data)\n",
    "    # print(CFD_size_data)\n",
    "    # print(underestimate)\n",
    "    # print(overestimate)\n",
    "    plt.scatter(predicted_size_data, CFD_size_data, facecolors='none', edgecolors='deepskyblue')\n",
    "    x = np.linspace(0, 20, 200)\n",
    "    plt.plot(x, x, '-r')\n",
    "    plt.xlabel('NN Predicted STD/nm')\n",
    "    plt.ylabel('CFD Predicted STD/nm')\n",
    "    plt.title('All Helical Size 4 Inputs')\n",
    "    plt.tick_params(axis='both', direction='in')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#save the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mChappyyyyyy\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPartIIB project 2023_2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMultiInput\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msize_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), PATH)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "# PATH = r'C:\\Users\\Chappyyyyyy\\Desktop\\PartIIB project 2023_2024\\MultiInput\\size_model.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#archived \n",
    "# class ConvNet(nn.Module): # note need to find out image size\n",
    "#     def __init__(self):\n",
    "#         super(ConvNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1,30,20, padding='same') #in_channels, out_channels, kernel_size\n",
    "#         self.normalise1 = nn.BatchNorm2d(30)\n",
    "#         # self.pool = nn.AvgPool2d(5,5) #kernel_size, stride (shift x pixel to the right)\n",
    "#         self.pool1 = nn.AvgPool2d(10, stride=10)\n",
    "#         # self.pool1 = nn.MaxPool2d(10, stride=10)\n",
    "#         self.conv2 = nn.Conv2d(30, 30, 20, padding='same')\n",
    "#         # self.normalise2 = nn.BatchNorm2d(16)\n",
    "#         # self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "#         self.pool2 = nn.MaxPool2d(2, stride=2)\n",
    "#         self.conv3 = nn.Conv2d(1, 30, 10, padding='same')\n",
    "#         # self.normalise3 = nn.BatchNorm2d(32) \n",
    "#         self.conv4 = nn.Conv2d(30, 30, 10, padding='same')\n",
    "#         self.fc0 = nn.Linear(30, 10)\n",
    "#         self.fc = nn.Linear(32*5*5, 1)\n",
    "#         self.fc1 = nn.Linear(60, 40)\n",
    "#         self.fc2 = nn.Linear(40,20)\n",
    "#         self.fc3 = nn.Linear(20,4)\n",
    "#         self.dropout = nn.Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = ConvNet().to(device)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, len(param.data))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
